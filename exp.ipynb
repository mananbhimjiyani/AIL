{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df10deac",
      "metadata": {
        "id": "df10deac"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 1: RECURSIVE DFS (KEYWORD: REC_DFS)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# Depth-First Search (DFS) is a graph traversal algorithm that explores as far as possible\n",
        "# along each branch before backtracking. In the recursive implementation:\n",
        "# 1. Start at a node and mark it as visited\n",
        "# 2. Recursively visit all unvisited neighbors\n",
        "# 3. Backtrack when no unvisited neighbors remain\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Create a recursive function that takes the graph, current node, and visited set\n",
        "# 2. Mark current node as visited and process it\n",
        "# 3. For each neighbor of current node:\n",
        "#    a. If neighbor is not visited, recursively call the function for that neighbor\n",
        "# 4. Return when all neighbors are processed\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def dfs_recursive(graph, node, visited):\n",
        "    visited.add(node)\n",
        "    print(node, end=\" \")  # Process the node\n",
        "    for neighbor in graph[node]:\n",
        "        if neighbor not in visited:\n",
        "            dfs_recursive(graph, neighbor, visited)\n",
        "\n",
        "# Read graph from CSV\n",
        "df = pd.read_csv('a.csv', header=None)\n",
        "graph = {}\n",
        "for row in df.values:\n",
        "    u, v = row\n",
        "    graph.setdefault(u, []).append(v)\n",
        "    graph.setdefault(v, []).append(u)\n",
        "\n",
        "# Perform DFS\n",
        "start_node = list(graph.keys())[0]  # Start from the first node\n",
        "visited = set()\n",
        "dfs_recursive(graph, start_node, visited)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f061833",
      "metadata": {
        "id": "6f061833"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 2: NON-RECURSIVE DFS (KEYWORD: STACK_DFS)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# Non-recursive DFS uses a stack data structure instead of the call stack.\n",
        "# This approach is more memory-efficient for large graphs as it avoids\n",
        "# stack overflow errors that might occur with the recursive approach.\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Create a stack and push the starting node\n",
        "# 2. Create a visited set to track visited nodes\n",
        "# 3. While stack is not empty:\n",
        "#    a. Pop a node from the stack\n",
        "#    b. If node is not visited, mark it as visited and process it\n",
        "#    c. Push all unvisited neighbors to the stack\n",
        "# 4. Return when stack is empty\n",
        "\n",
        "def dfs_non_recursive(graph, start):\n",
        "    stack = [start]\n",
        "    visited = set()\n",
        "    while stack:\n",
        "        node = stack.pop()\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            print(node, end=\" \")\n",
        "            # Add neighbors in reverse order to maintain left-to-right traversal\n",
        "            stack.extend(reversed(graph[node]))\n",
        "\n",
        "# Input graph from user\n",
        "graph = {}\n",
        "num_edges = int(input(\"Enter number of edges: \"))\n",
        "for _ in range(num_edges):\n",
        "    u, v = input(\"Enter edge (u v): \").split()\n",
        "    graph.setdefault(u, []).append(v)\n",
        "    graph.setdefault(v, []).append(u)\n",
        "\n",
        "# Perform DFS\n",
        "start_node = input(\"Enter start node: \")\n",
        "dfs_non_recursive(graph, start_node)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99daa0b0",
      "metadata": {
        "id": "99daa0b0"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 3: BFS (KEYWORD: BFS)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# Breadth-First Search (BFS) traverses a graph level by level. It visits all\n",
        "# neighbors at the current level before moving to nodes at the next level.\n",
        "# BFS is useful for finding the shortest path in unweighted graphs.\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Create a queue and enqueue the starting node\n",
        "# 2. Create a visited set to track visited nodes\n",
        "# 3. While queue is not empty:\n",
        "#    a. Dequeue a node from the queue\n",
        "#    b. If node is not visited, mark it as visited and process it\n",
        "#    c. Enqueue all unvisited neighbors\n",
        "# 4. Return when queue is empty\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "def bfs(graph, start):\n",
        "    queue = deque([start])\n",
        "    visited = set()\n",
        "    while queue:\n",
        "        node = queue.popleft()\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            print(node, end=\" \")\n",
        "            queue.extend(neighbor for neighbor in graph[node] if neighbor not in visited)\n",
        "\n",
        "# Input graph from user\n",
        "graph = {}\n",
        "num_edges = int(input(\"Enter number of edges: \"))\n",
        "for _ in range(num_edges):\n",
        "    u, v = input(\"Enter edge (u v): \").split()\n",
        "    graph.setdefault(u, []).append(v)\n",
        "    graph.setdefault(v, []).append(u)\n",
        "\n",
        "# Perform BFS\n",
        "start_node = input(\"Enter start node: \")\n",
        "bfs(graph, start_node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47f8d5a9",
      "metadata": {
        "id": "47f8d5a9"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 4: BEST FIRST SEARCH - DIRECTED UNWEIGHTED (KEYWORD: BFS_DIR_UNW)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# Best First Search is a search algorithm that selects the path which appears\n",
        "# best based on a heuristic or evaluation function. It uses a priority queue\n",
        "# where nodes with better heuristic values are explored first.\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Create a priority queue and add the starting node with its heuristic value\n",
        "# 2. Create a visited set to track visited nodes\n",
        "# 3. While priority queue is not empty:\n",
        "#    a. Remove node with the best heuristic value\n",
        "#    b. If node is not visited, mark it as visited and process it\n",
        "#    c. Add all unvisited neighbors to the priority queue with their heuristic values\n",
        "# 4. Return when priority queue is empty\n",
        "\n",
        "import heapq\n",
        "\n",
        "def best_first_search_directed_unweighted(graph, start, heuristic):\n",
        "    priority_queue = [(heuristic[start], start)]\n",
        "    visited = set()\n",
        "    while priority_queue:\n",
        "        _, node = heapq.heappop(priority_queue)\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            print(node, end=\" \")\n",
        "            for neighbor in graph[node]:\n",
        "                if neighbor not in visited:\n",
        "                    heapq.heappush(priority_queue, (heuristic[neighbor], neighbor))\n",
        "\n",
        "# Input directed unweighted graph and heuristic values from user\n",
        "graph = {}\n",
        "heuristic = {}\n",
        "num_edges = int(input(\"Enter number of edges: \"))\n",
        "for _ in range(num_edges):\n",
        "    u, v = input(\"Enter edge (u v): \").split()\n",
        "    graph.setdefault(u, []).append(v)\n",
        "    # Note: No reverse edge since it's directed\n",
        "\n",
        "nodes = set(graph.keys())\n",
        "for node in nodes:\n",
        "    heuristic[node] = float(input(f\"Enter heuristic value for {node}: \"))\n",
        "\n",
        "# Perform Best First Search\n",
        "start_node = input(\"Enter start node: \")\n",
        "best_first_search_directed_unweighted(graph, start_node, heuristic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f5d7810",
      "metadata": {
        "id": "8f5d7810"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 5: BEST FIRST SEARCH - UNDIRECTED WEIGHTED (KEYWORD: BFS_UNDIR_W)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# This version of Best First Search works with undirected weighted graphs.\n",
        "# The weights don't affect the search order directly (that would be Dijkstra's),\n",
        "# but we store them for potential use in applications.\n",
        "#\n",
        "# Algorithm:\n",
        "# Same as Experiment 4, but we add edges in both directions for undirected graph\n",
        "# and store weights (though they don't affect the search order).\n",
        "\n",
        "import heapq\n",
        "\n",
        "def best_first_search_undirected_weighted(graph, start, heuristic):\n",
        "    priority_queue = [(heuristic[start], start)]\n",
        "    visited = set()\n",
        "    while priority_queue:\n",
        "        _, node = heapq.heappop(priority_queue)\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            print(node, end=\" \")\n",
        "            for neighbor, weight in graph[node]:\n",
        "                if neighbor not in visited:\n",
        "                    heapq.heappush(priority_queue, (heuristic[neighbor], neighbor))\n",
        "\n",
        "# Input undirected weighted graph and heuristic values from user\n",
        "graph = {}\n",
        "heuristic = {}\n",
        "num_edges = int(input(\"Enter number of edges: \"))\n",
        "for _ in range(num_edges):\n",
        "    u, v, weight = input(\"Enter edge (u v weight): \").split()\n",
        "    weight = float(weight)\n",
        "    graph.setdefault(u, []).append((v, weight))\n",
        "    graph.setdefault(v, []).append((u, weight))  # Add reverse edge for undirected\n",
        "\n",
        "nodes = set()\n",
        "for node in graph:\n",
        "    nodes.add(node)\n",
        "    for neighbor, _ in graph[node]:\n",
        "        nodes.add(neighbor)\n",
        "\n",
        "for node in nodes:\n",
        "    heuristic[node] = float(input(f\"Enter heuristic value for {node}: \"))\n",
        "\n",
        "# Perform Best First Search\n",
        "start_node = input(\"Enter start node: \")\n",
        "best_first_search_undirected_weighted(graph, start_node, heuristic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d0cc6f",
      "metadata": {
        "id": "94d0cc6f"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 6: BEST FIRST SEARCH - UNDIRECTED UNWEIGHTED (KEYWORD: BFS_UNDIR_UNW)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# Best First Search for undirected unweighted graphs is a simpler version\n",
        "# where we just consider connectivity without weights.\n",
        "#\n",
        "# Algorithm:\n",
        "# Similar to Experiment 4, but with edges in both directions for undirected graph.\n",
        "\n",
        "import heapq\n",
        "\n",
        "def best_first_search_undirected_unweighted(graph, start, heuristic):\n",
        "    priority_queue = [(heuristic[start], start)]\n",
        "    visited = set()\n",
        "    while priority_queue:\n",
        "        _, node = heapq.heappop(priority_queue)\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            print(node, end=\" \")\n",
        "            for neighbor in graph[node]:\n",
        "                if neighbor not in visited:\n",
        "                    heapq.heappush(priority_queue, (heuristic[neighbor], neighbor))\n",
        "\n",
        "# Input undirected unweighted graph and heuristic values from user\n",
        "graph = {}\n",
        "heuristic = {}\n",
        "num_edges = int(input(\"Enter number of edges: \"))\n",
        "for _ in range(num_edges):\n",
        "    u, v = input(\"Enter edge (u v): \").split()\n",
        "    graph.setdefault(u, []).append(v)\n",
        "    graph.setdefault(v, []).append(u)  # Add reverse edge for undirected\n",
        "\n",
        "nodes = set(graph.keys())\n",
        "for node in nodes:\n",
        "    heuristic[node] = float(input(f\"Enter heuristic value for {node}: \"))\n",
        "\n",
        "# Perform Best First Search\n",
        "start_node = input(\"Enter start node: \")\n",
        "best_first_search_undirected_unweighted(graph, start_node, heuristic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5092f2b9",
      "metadata": {
        "id": "5092f2b9"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 7: BEST FIRST SEARCH - DIRECTED WEIGHTED (KEYWORD: BFS_DIR_W)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# Best First Search for directed weighted graphs considers direction and\n",
        "# stores weights (though weights don't affect search order in basic Best First Search).\n",
        "#\n",
        "# Algorithm:\n",
        "# Similar to Experiment 5, but without adding reverse edges.\n",
        "\n",
        "import heapq\n",
        "\n",
        "def best_first_search_directed_weighted(graph, start, heuristic):\n",
        "    priority_queue = [(heuristic[start], start)]\n",
        "    visited = set()\n",
        "    while priority_queue:\n",
        "        _, node = heapq.heappop(priority_queue)\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            print(node, end=\" \")\n",
        "            for neighbor, weight in graph[node]:\n",
        "                if neighbor not in visited:\n",
        "                    heapq.heappush(priority_queue, (heuristic[neighbor], neighbor))\n",
        "\n",
        "# Input directed weighted graph and heuristic values from user\n",
        "graph = {}\n",
        "heuristic = {}\n",
        "num_edges = int(input(\"Enter number of edges: \"))\n",
        "for _ in range(num_edges):\n",
        "    u, v, weight = input(\"Enter edge (u v weight): \").split()\n",
        "    weight = float(weight)\n",
        "    graph.setdefault(u, []).append((v, weight))\n",
        "    # No reverse edge since it's directed\n",
        "\n",
        "nodes = set()\n",
        "for node in graph:\n",
        "    nodes.add(node)\n",
        "    for neighbor, _ in graph[node]:\n",
        "        nodes.add(neighbor)\n",
        "\n",
        "for node in nodes:\n",
        "    heuristic[node] = float(input(f\"Enter heuristic value for {node}: \"))\n",
        "\n",
        "# Perform Best First Search\n",
        "start_node = input(\"Enter start node: \")\n",
        "best_first_search_directed_weighted(graph, start_node, heuristic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a45072",
      "metadata": {
        "id": "48a45072"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 8: A* ALGORITHM - DIRECTED WEIGHTED FROM CSV (KEYWORD: ASTAR_DIR_W_CSV)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# A* algorithm combines the advantages of Dijkstra's algorithm and Best First Search.\n",
        "# It uses both the cost to reach a node (g(n)) and the heuristic estimate to the goal (h(n))\n",
        "# to determine the order of node exploration: f(n) = g(n) + h(n).\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Create a priority queue and add the starting node with f(n) = g(n) + h(n)\n",
        "# 2. Create a visited set to track visited nodes\n",
        "# 3. While priority queue is not empty:\n",
        "#    a. Remove node with the lowest f(n) value\n",
        "#    b. If node is goal, return success\n",
        "#    c. If node is not visited, mark it as visited and process it\n",
        "#    d. For each neighbor, calculate g(neighbor) = g(current) + cost(current, neighbor)\n",
        "#    e. Add neighbor to priority queue with f(n) = g(neighbor) + h(neighbor)\n",
        "# 4. Return failure if goal not found\n",
        "\n",
        "import heapq\n",
        "import pandas as pd\n",
        "\n",
        "def a_star_directed_weighted_csv(graph, start, goal, heuristic):\n",
        "    priority_queue = [(0 + heuristic[start], 0, start)]  # (f(n), g(n), node)\n",
        "    visited = set()\n",
        "    while priority_queue:\n",
        "        f_n, g_n, node = heapq.heappop(priority_queue)\n",
        "        if node == goal:\n",
        "            print(f\"Found goal: {goal}\")\n",
        "            return\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            print(node, end=\" \")\n",
        "            for neighbor, cost in graph[node]:\n",
        "                if neighbor not in visited:\n",
        "                    heapq.heappush(priority_queue, (g_n + cost + heuristic[neighbor], g_n + cost, neighbor))\n",
        "\n",
        "# Read directed weighted graph and heuristic from CSV\n",
        "df_edges = pd.read_csv('graph_edges.csv', header=None)  # should have columns: source, target, weight\n",
        "df_heuristic = pd.read_csv('heuristic.csv', header=None)  # should have columns: node, heuristic_value\n",
        "\n",
        "graph = {}\n",
        "heuristic = {}\n",
        "\n",
        "for row in df_edges.values:\n",
        "    u, v, weight = row\n",
        "    weight = float(weight)\n",
        "    graph.setdefault(u, []).append((v, weight))\n",
        "\n",
        "for row in df_heuristic.values:\n",
        "    node, h_value = row\n",
        "    heuristic[node] = float(h_value)\n",
        "\n",
        "# Perform A* Search\n",
        "start_node = input(\"Enter start node: \")\n",
        "goal_node = input(\"Enter goal node: \")\n",
        "a_star_directed_weighted_csv(graph, start_node, goal_node, heuristic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af13a67f",
      "metadata": {
        "id": "af13a67f"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 9: A* ALGORITHM - DIRECTED WEIGHTED FROM USER (KEYWORD: ASTAR_DIR_W_USER)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# Same as Experiment 8, but with manual input from user rather than CSV files.\n",
        "#\n",
        "# Algorithm:\n",
        "# Same A* algorithm but with different input method.\n",
        "\n",
        "import heapq\n",
        "\n",
        "def a_star_directed_weighted_user(graph, start, goal, heuristic):\n",
        "    priority_queue = [(0 + heuristic[start], 0, start)]  # (f(n), g(n), node)\n",
        "    visited = set()\n",
        "    while priority_queue:\n",
        "        f_n, g_n, node = heapq.heappop(priority_queue)\n",
        "        if node == goal:\n",
        "            print(f\"Found goal: {goal}\")\n",
        "            return\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            print(node, end=\" \")\n",
        "            for neighbor, cost in graph[node]:\n",
        "                if neighbor not in visited:\n",
        "                    heapq.heappush(priority_queue, (g_n + cost + heuristic[neighbor], g_n + cost, neighbor))\n",
        "\n",
        "# Input directed weighted graph and heuristic values from user\n",
        "graph = {}\n",
        "heuristic = {}\n",
        "num_edges = int(input(\"Enter number of edges: \"))\n",
        "for _ in range(num_edges):\n",
        "    u, v, cost = input(\"Enter edge (u v cost): \").split()\n",
        "    cost = float(cost)\n",
        "    graph.setdefault(u, []).append((v, cost))\n",
        "    # No reverse edge since it's directed\n",
        "\n",
        "nodes = set()\n",
        "for node in graph:\n",
        "    nodes.add(node)\n",
        "    for neighbor, _ in graph[node]:\n",
        "        nodes.add(neighbor)\n",
        "\n",
        "for node in nodes:\n",
        "    heuristic[node] = float(input(f\"Enter heuristic value for {node}: \"))\n",
        "\n",
        "# Perform A* Search\n",
        "start_node = input(\"Enter start node: \")\n",
        "goal_node = input(\"Enter goal node: \")\n",
        "a_star_directed_weighted_user(graph, start_node, goal_node, heuristic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "579b2ccc",
      "metadata": {
        "id": "579b2ccc"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 10: A* ALGORITHM - UNDIRECTED WEIGHTED FROM CSV (KEYWORD: ASTAR_UNDIR_W_CSV)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# A* algorithm for undirected weighted graphs, reading data from CSV files.\n",
        "#\n",
        "# Algorithm:\n",
        "# Same as Experiment 8, but treating the graph as undirected.\n",
        "\n",
        "import heapq\n",
        "import pandas as pd\n",
        "\n",
        "def a_star_undirected_weighted_csv(graph, start, goal, heuristic):\n",
        "    priority_queue = [(0 + heuristic[start], 0, start)]  # (f(n), g(n), node)\n",
        "    visited = set()\n",
        "    while priority_queue:\n",
        "        f_n, g_n, node = heapq.heappop(priority_queue)\n",
        "        if node == goal:\n",
        "            print(f\"Found goal: {goal}\")\n",
        "            return\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            print(node, end=\" \")\n",
        "            for neighbor, cost in graph[node]:\n",
        "                if neighbor not in visited:\n",
        "                    heapq.heappush(priority_queue, (g_n + cost + heuristic[neighbor], g_n + cost, neighbor))\n",
        "\n",
        "# Read undirected weighted graph and heuristic from CSV\n",
        "df_edges = pd.read_csv('graph_edges.csv', header=None)  # columns: source, target, weight\n",
        "df_heuristic = pd.read_csv('heuristic.csv', header=None)  # columns: node, heuristic_value\n",
        "\n",
        "graph = {}\n",
        "heuristic = {}\n",
        "\n",
        "for row in df_edges.values:\n",
        "    u, v, weight = row\n",
        "    weight = float(weight)\n",
        "    graph.setdefault(u, []).append((v, weight))\n",
        "    graph.setdefault(v, []).append((u, weight))  # Add reverse edge for undirected\n",
        "\n",
        "for row in df_heuristic.values:\n",
        "    node, h_value = row\n",
        "    heuristic[node] = float(h_value)\n",
        "\n",
        "# Perform A* Search\n",
        "start_node = input(\"Enter start node: \")\n",
        "goal_node = input(\"Enter goal node: \")\n",
        "a_star_undirected_weighted_csv(graph, start_node, goal_node, heuristic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da03467",
      "metadata": {
        "id": "1da03467"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 11: A* ALGORITHM - UNDIRECTED WEIGHTED FROM USER (KEYWORD: ASTAR_UNDIR_W_USER)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# A* algorithm for undirected weighted graphs with manual input from user.\n",
        "#\n",
        "# Algorithm:\n",
        "# Same as Experiment 9, but treating the graph as undirected.\n",
        "\n",
        "import heapq\n",
        "\n",
        "def a_star_undirected_weighted_user(graph, start, goal, heuristic):\n",
        "    priority_queue = [(0 + heuristic[start], 0, start)]  # (f(n), g(n), node)\n",
        "    visited = set()\n",
        "    while priority_queue:\n",
        "        f_n, g_n, node = heapq.heappop(priority_queue)\n",
        "        if node == goal:\n",
        "            print(f\"Found goal: {goal}\")\n",
        "            return\n",
        "        if node not in visited:\n",
        "            visited.add(node)\n",
        "            print(node, end=\" \")\n",
        "            for neighbor, cost in graph[node]:\n",
        "                if neighbor not in visited:\n",
        "                    heapq.heappush(priority_queue, (g_n + cost + heuristic[neighbor], g_n + cost, neighbor))\n",
        "\n",
        "# Input undirected weighted graph and heuristic values from user\n",
        "graph = {}\n",
        "heuristic = {}\n",
        "num_edges = int(input(\"Enter number of edges: \"))\n",
        "for _ in range(num_edges):\n",
        "    u, v, cost = input(\"Enter edge (u v cost): \").split()\n",
        "    cost = float(cost)\n",
        "    graph.setdefault(u, []).append((v, cost))\n",
        "    graph.setdefault(v, []).append((u, cost))  # Add reverse edge for undirected\n",
        "\n",
        "nodes = set()\n",
        "for node in graph:\n",
        "    nodes.add(node)\n",
        "    for neighbor, _ in graph[node]:\n",
        "        nodes.add(neighbor)\n",
        "\n",
        "for node in nodes:\n",
        "    heuristic[node] = float(input(f\"Enter heuristic value for {node}: \"))\n",
        "\n",
        "# Perform A* Search\n",
        "start_node = input(\"Enter start node: \")\n",
        "goal_node = input(\"Enter goal node: \")\n",
        "a_star_undirected_weighted_user(graph, start_node, goal_node, heuristic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "490d4270",
      "metadata": {
        "id": "490d4270"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 12: FUZZY SET OPERATIONS (KEYWORD: FUZZY_BASIC)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# Fuzzy sets are sets whose elements have degrees of membership between 0 and 1.\n",
        "# Basic operations on fuzzy sets include:\n",
        "# - Union: max(μA(x), μB(x))\n",
        "# - Intersection: min(μA(x), μB(x))\n",
        "# - Complement: 1 - μA(x)\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Define fuzzy sets as dictionaries with elements as keys and membership degrees as values\n",
        "# 2. Implement union by taking the maximum membership degree for each element\n",
        "# 3. Implement intersection by taking the minimum membership degree for each element\n",
        "# 4. Implement complement by subtracting each membership degree from 1\n",
        "\n",
        "# Define three fuzzy sets\n",
        "A = {\"a\": 0.7, \"b\": 0.4, \"c\": 0.9}\n",
        "B = {\"a\": 0.5, \"b\": 0.6, \"c\": 0.3}\n",
        "C = {\"a\": 0.8, \"b\": 0.2, \"c\": 0.5}\n",
        "\n",
        "# Union\n",
        "union_AB = {key: max(A.get(key, 0), B.get(key, 0)) for key in set(A).union(B)}\n",
        "print(\"Union of A and B:\", union_AB)\n",
        "\n",
        "# Intersection\n",
        "intersection_AB = {key: min(A.get(key, 0), B.get(key, 0)) for key in set(A).intersection(B)}\n",
        "print(\"Intersection of A and B:\", intersection_AB)\n",
        "\n",
        "# Complement\n",
        "complement_A = {key: 1 - value for key, value in A.items()}\n",
        "print(\"Complement of A:\", complement_A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d3299b2",
      "metadata": {
        "id": "7d3299b2"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 13: FUZZY SET - DE MORGAN'S LAW (UNION) (KEYWORD: FUZZY_DEMORGAN1)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# De Morgan's Law for fuzzy sets states that:\n",
        "# Complement of Union: ¬(A ∪ B) = ¬A ∩ ¬B\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Compute complement of union: 1 - max(μA(x), μB(x))\n",
        "# 2. Compute intersection of complements: min(1 - μA(x), 1 - μB(x))\n",
        "# 3. Verify that both results are equal\n",
        "\n",
        "# Define two fuzzy sets\n",
        "A = {\"a\": 0.7, \"b\": 0.4, \"c\": 0.9}\n",
        "B = {\"a\": 0.5, \"b\": 0.6, \"c\": 0.3}\n",
        "\n",
        "# Complement of Union\n",
        "complement_union = {key: 1 - max(A.get(key, 0), B.get(key, 0)) for key in set(A).union(B)}\n",
        "\n",
        "# Intersection of Complements\n",
        "intersection_complements = {\n",
        "    key: min(1 - A.get(key, 0), 1 - B.get(key, 0)) for key in set(A).union(B)\n",
        "}\n",
        "\n",
        "# Verify De Morgan's Law\n",
        "print(\"Complement of Union:\", complement_union)\n",
        "print(\"Intersection of Complements:\", intersection_complements)\n",
        "print(\"De Morgan's Law Verified:\", complement_union == intersection_complements)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab11bf33",
      "metadata": {
        "id": "ab11bf33"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 14: FUZZY SET - DE MORGAN'S LAW (INTERSECTION) (KEYWORD: FUZZY_DEMORGAN2)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# De Morgan's Law for fuzzy sets also states that:\n",
        "# Complement of Intersection: ¬(A ∩ B) = ¬A ∪ ¬B\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Compute complement of intersection: 1 - min(μA(x), μB(x))\n",
        "# 2. Compute union of complements: max(1 - μA(x), 1 - μB(x))\n",
        "# 3. Verify that both results are equal\n",
        "\n",
        "# Define two fuzzy sets\n",
        "A = {\"a\": 0.7, \"b\": 0.4, \"c\": 0.9}\n",
        "B = {\"a\": 0.5, \"b\": 0.6, \"c\": 0.3}\n",
        "\n",
        "# Complement of Intersection\n",
        "complement_intersection = {key: 1 - min(A.get(key, 0), B.get(key, 0)) for key in set(A).intersection(B)}\n",
        "\n",
        "# Union of Complements\n",
        "union_complements = {\n",
        "    key: max(1 - A.get(key, 0), 1 - B.get(key, 0)) for key in set(A).union(B)\n",
        "}\n",
        "\n",
        "# Verify De Morgan's Law\n",
        "print(\"Complement of Intersection:\", complement_intersection)\n",
        "print(\"Union of Complements:\", union_complements)\n",
        "print(\"De Morgan's Law Verified:\", complement_intersection == union_complements)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70b0339f",
      "metadata": {
        "id": "70b0339f"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 15: MINIMAX - NIM GAME (WIN OR DRAW) (KEYWORD: MINIMAX_WIN)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# Minimax is a decision rule for minimizing the possible loss in a worst-case scenario.\n",
        "# In the Nim game, players take turns removing 1-3 stones from a pile, and whoever\n",
        "# takes the last stone wins. The minimax algorithm helps the computer make optimal moves.\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Define a minimax function that evaluates game states recursively:\n",
        "#    a. If terminal state (no stones left), return score (+1 for win, -1 for loss)\n",
        "#    b. For maximizing player: choose the maximum score of all possible moves\n",
        "#    c. For minimizing player: choose the minimum score of all possible moves\n",
        "# 2. For each possible move, calculate the score using minimax\n",
        "# 3. Choose the move with the highest score\n",
        "\n",
        "def minimax_nim(stones, is_maximizing):\n",
        "    # Base case: if no stones left, determine the winner\n",
        "    if stones == 0:\n",
        "        return -1 if is_maximizing else 1  # Loss for maximizer, win for minimizer\n",
        "\n",
        "    # Try all possible moves (take 1, 2, or 3 stones)\n",
        "    if is_maximizing:\n",
        "        best_score = -float('inf')\n",
        "        for move in range(1, min(4, stones + 1)):  # Take 1, 2, or 3 stones\n",
        "            score = minimax_nim(stones - move, False)\n",
        "            best_score = max(best_score, score)\n",
        "        return best_score\n",
        "    else:\n",
        "        best_score = float('inf')\n",
        "        for move in range(1, min(4, stones + 1)):\n",
        "            score = minimax_nim(stones - move, True)\n",
        "            best_score = min(best_score, score)\n",
        "        return best_score\n",
        "\n",
        "def find_best_move_nim(stones):\n",
        "    best_move = -1\n",
        "    best_score = -float('inf')\n",
        "    for move in range(1, min(4, stones + 1)):  # Try taking 1, 2, or 3 stones\n",
        "        score = minimax_nim(stones - move, False)  # Opponent's turn\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_move = move\n",
        "    return best_move\n",
        "\n",
        "# Play the game\n",
        "stones = 10  # Initial number of stones\n",
        "print(f\"Game starts with {stones} stones.\")\n",
        "\n",
        "while stones > 0:\n",
        "    # Computer's turn\n",
        "    move = find_best_move_nim(stones)\n",
        "    print(f\"Computer takes {move} stones.\")\n",
        "    stones -= move\n",
        "    if stones <= 0:\n",
        "        print(\"Computer wins!\")\n",
        "        break\n",
        "\n",
        "    # Player's turn\n",
        "    player_move = int(input(f\"{stones} stones left. How many do you take (1-3)? \"))\n",
        "    while player_move < 1 or player_move > 3 or player_move > stones:\n",
        "        print(\"Invalid move. Try again.\")\n",
        "        player_move = int(input(f\"{stones} stones left. How many do you take (1-3)? \"))\n",
        "    stones -= player_move\n",
        "    if stones <= 0:\n",
        "        print(\"You win!\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 16: MINIMAX - NIM GAME (LOSE OR DRAW) (KEYWORD: MINIMAX_LOSE)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# Minimax is a decision rule for minimizing the possible loss in a worst-case scenario.\n",
        "# In the Nim game, players take turns removing 1-3 stones from a pile, and whoever\n",
        "# takes the last stone wins. This implementation modifies the minimax algorithm to ensure\n",
        "# the computer either loses or draws - never wins.\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Define a minimax function that evaluates game states recursively:\n",
        "#    a. If terminal state (no stones left), return score (-1 for loss, +1 for win)\n",
        "#    b. For maximizing player: choose the maximum score of all possible moves\n",
        "#    c. For minimizing player: choose the minimum score of all possible moves\n",
        "# 2. For each possible move, calculate the score using minimax\n",
        "# 3. Choose the move with the LOWEST score to make the computer lose if possible\n",
        "#    (or draw if losing is not possible)\n",
        "\n",
        "def minimax_nim(stones, is_maximizing):\n",
        "    # Base case: if no stones left, determine the winner\n",
        "    if stones == 0:\n",
        "        return -1 if is_maximizing else 1  # Loss for maximizer, win for minimizer\n",
        "\n",
        "    # Try all possible moves (take 1, 2, or 3 stones)\n",
        "    if is_maximizing:\n",
        "        best_score = -float('inf')\n",
        "        for move in range(1, min(4, stones + 1)):  # Take 1, 2, or 3 stones\n",
        "            score = minimax_nim(stones - move, False)\n",
        "            best_score = max(best_score, score)\n",
        "        return best_score\n",
        "    else:\n",
        "        best_score = float('inf')\n",
        "        for move in range(1, min(4, stones + 1)):\n",
        "            score = minimax_nim(stones - move, True)\n",
        "            best_score = min(best_score, score)\n",
        "        return best_score\n",
        "\n",
        "def find_worst_move_nim(stones):\n",
        "    # We're looking for a move that leads to a loss (or draw if no loss is possible)\n",
        "    best_move = -1\n",
        "    best_score = float('inf')  # We want the LOWEST score, not highest\n",
        "\n",
        "    for move in range(1, min(4, stones + 1)):  # Try taking 1, 2, or 3 stones\n",
        "        score = minimax_nim(stones - move, False)  # Opponent's turn\n",
        "        if score < best_score:\n",
        "            best_score = score\n",
        "            best_move = move\n",
        "\n",
        "    # If we can't find a losing move, just make any valid move\n",
        "    if best_move == -1 and stones > 0:\n",
        "        best_move = 1\n",
        "\n",
        "    return best_move\n",
        "\n",
        "# Play the game\n",
        "stones = 10  # Initial number of stones\n",
        "print(f\"Game starts with {stones} stones.\")\n",
        "print(\"In this game, the player who takes the last stone wins!\")\n",
        "\n",
        "while stones > 0:\n",
        "    # Computer's turn\n",
        "    move = find_worst_move_nim(stones)\n",
        "    print(f\"Computer takes {move} stones.\")\n",
        "    stones -= move\n",
        "    print(f\"{stones} stones remaining.\")\n",
        "\n",
        "    if stones <= 0:\n",
        "        print(\"Computer took the last stone. Computer loses!\")\n",
        "        break\n",
        "\n",
        "    # Player's turn\n",
        "    player_move = int(input(f\"How many stones do you take (1-3)? \"))\n",
        "    while player_move < 1 or player_move > 3 or player_move > stones:\n",
        "        print(\"Invalid move. Try again.\")\n",
        "        player_move = int(input(f\"How many stones do you take (1-3)? \"))\n",
        "\n",
        "    stones -= player_move\n",
        "    print(f\"{stones} stones remaining.\")\n",
        "\n",
        "    if stones <= 0:\n",
        "        print(\"You took the last stone. You lose!\")\n",
        "        break\n",
        "\n",
        "# Note: In this game, taking the last stone means you LOSE\n",
        "# The computer will always try to make moves that lead to its own loss if possible"
      ],
      "metadata": {
        "id": "1FR9BINWX_Ft"
      },
      "id": "1FR9BINWX_Ft",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 17: MLP - N BINARY INPUTS, TWO HIDDEN LAYERS, ONE OUTPUT (RANDOM WEIGHTS) (KEYWORD: MLP_RANDOM)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# A Multi-Layer Perceptron (MLP) is a class of feedforward artificial neural network.\n",
        "# This experiment implements an MLP with N binary inputs, two hidden layers, and one output.\n",
        "# The weights and biases are randomly initialized and not trained, demonstrating the initial\n",
        "# state of a neural network before training.\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Initialize random weights and biases for all layers\n",
        "# 2. Define the network architecture:\n",
        "#    a. Input layer with N binary inputs\n",
        "#    b. Two hidden layers with specified number of neurons\n",
        "#    c. One output layer with sigmoid activation\n",
        "# 3. Implement forward propagation:\n",
        "#    a. Compute weighted sum for each layer\n",
        "#    b. Apply activation function (sigmoid)\n",
        "# 4. Display the random weights and biases\n",
        "# 5. Test the network with all possible binary inputs\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MLP_Random:\n",
        "    def __init__(self, n_inputs, hidden1_size, hidden2_size):\n",
        "        self.n_inputs = n_inputs\n",
        "        self.hidden1_size = hidden1_size\n",
        "        self.hidden2_size = hidden2_size\n",
        "\n",
        "        # Initialize random weights and biases\n",
        "        self.weights1 = np.random.rand(n_inputs, hidden1_size)\n",
        "        self.bias1 = np.random.rand(1, hidden1_size)\n",
        "\n",
        "        self.weights2 = np.random.rand(hidden1_size, hidden2_size)\n",
        "        self.bias2 = np.random.rand(1, hidden2_size)\n",
        "\n",
        "        self.weights3 = np.random.rand(hidden2_size, 1)\n",
        "        self.bias3 = np.random.rand(1, 1)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # First hidden layer\n",
        "        self.z1 = np.dot(X, self.weights1) + self.bias1\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "\n",
        "        # Second hidden layer\n",
        "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "\n",
        "        # Output layer\n",
        "        self.z3 = np.dot(self.a2, self.weights3) + self.bias3\n",
        "        self.a3 = self.sigmoid(self.z3)\n",
        "\n",
        "        return self.a3\n",
        "\n",
        "def experiment17():\n",
        "    n_inputs = int(input(\"Enter number of binary inputs (N): \"))\n",
        "    hidden1_size = n_inputs + 2\n",
        "    hidden2_size = n_inputs\n",
        "\n",
        "    mlp = MLP_Random(n_inputs, hidden1_size, hidden2_size)\n",
        "\n",
        "    # Generate all possible binary input combinations\n",
        "    X = np.array([list(map(int, format(i, f'0{n_inputs}b'))) for i in range(2**n_inputs)])\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = mlp.forward(X)\n",
        "    predicted = (outputs >= 0.5).astype(int)\n",
        "\n",
        "    print(\"\\nFinal weights and biases:\")\n",
        "    print(\"Layer 1 Weights:\\n\", mlp.weights1)\n",
        "    print(\"Layer 1 Bias:\\n\", mlp.bias1)\n",
        "    print(\"\\nLayer 2 Weights:\\n\", mlp.weights2)\n",
        "    print(\"Layer 2 Bias:\\n\", mlp.bias2)\n",
        "    print(\"\\nOutput Layer Weights:\\n\", mlp.weights3)\n",
        "    print(\"Output Layer Bias:\\n\", mlp.bias3)\n",
        "\n",
        "    print(\"\\nTesting the network:\")\n",
        "    for i, inputs in enumerate(X):\n",
        "        print(f\"Input: {inputs}, Output: {outputs[i][0]:.4f}, Predicted: {predicted[i][0]}\")\n",
        "\n",
        "experiment17()"
      ],
      "metadata": {
        "id": "jpJ51b03aGQd"
      },
      "id": "jpJ51b03aGQd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 18: MLP - 4 BINARY INPUTS, ONE HIDDEN LAYER, TWO OUTPUTS (KEYWORD: MLP_4IN_2OUT)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# This MLP variation has a fixed architecture with 4 binary inputs, one hidden layer,\n",
        "# and two binary outputs. The network demonstrates how multiple outputs can be learned\n",
        "# simultaneously, with each output representing a different binary classification task.\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Initialize random weights and biases for:\n",
        "#    a. Input to hidden layer connections\n",
        "#    b. Hidden to output layer connections\n",
        "# 2. Implement forward propagation:\n",
        "#    a. Compute hidden layer activations using sigmoid\n",
        "#    b. Compute output layer activations using sigmoid\n",
        "# 3. Display the network architecture and parameters\n",
        "# 4. Test with all possible 4-bit input combinations\n",
        "import numpy as np\n",
        "\n",
        "class MLP_4Inputs_2Outputs:\n",
        "    def __init__(self, hidden_size):\n",
        "        self.n_inputs = 4\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_outputs = 2\n",
        "\n",
        "        # Initialize random weights and biases\n",
        "        self.weights1 = np.random.rand(self.n_inputs, hidden_size)\n",
        "        self.bias1 = np.random.rand(1, hidden_size)\n",
        "\n",
        "        self.weights2 = np.random.rand(hidden_size, self.n_outputs)\n",
        "        self.bias2 = np.random.rand(1, self.n_outputs)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Hidden layer\n",
        "        self.z1 = np.dot(X, self.weights1) + self.bias1\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "\n",
        "        # Output layer\n",
        "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "\n",
        "        return self.a2\n",
        "\n",
        "def experiment18():\n",
        "    hidden_size = 6  # Can be adjusted\n",
        "\n",
        "    mlp = MLP_4Inputs_2Outputs(hidden_size)\n",
        "\n",
        "    # Generate all possible binary input combinations for 4 inputs\n",
        "    X = np.array([list(map(int, format(i, '04b'))) for i in range(16)])\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = mlp.forward(X)\n",
        "    predicted = (outputs >= 0.5).astype(int)\n",
        "\n",
        "    print(\"\\nFinal weights and biases:\")\n",
        "    print(\"Hidden Layer Weights:\\n\", mlp.weights1)\n",
        "    print(\"Hidden Layer Bias:\\n\", mlp.bias1)\n",
        "    print(\"\\nOutput Layer Weights:\\n\", mlp.weights2)\n",
        "    print(\"Output Layer Bias:\\n\", mlp.bias2)\n",
        "\n",
        "    print(\"\\nTesting the network:\")\n",
        "    for i, inputs in enumerate(X):\n",
        "        print(f\"Input: {inputs}, Outputs: {outputs[i]}, Predicted: {predicted[i]}\")\n",
        "\n",
        "experiment18()"
      ],
      "metadata": {
        "id": "MEfWaYazaKT3"
      },
      "id": "MEfWaYazaKT3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 19: MULTI-LAYER PERCEPTRON - N BINARY INPUTS, TWO HIDDEN LAYERS, ONE OUTPUT (SIGMOID)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# This experiment implements a Multi-Layer Perceptron with N binary inputs, two hidden layers,\n",
        "# and one output. Backpropagation is used to train the network with the Sigmoid function\n",
        "# as the activation function.\n",
        "#\n",
        "# Algorithm (Backpropagation):\n",
        "# 1. Initialize random weights and biases\n",
        "# 2. Forward pass: Compute output of the network\n",
        "# 3. Compute error at the output layer\n",
        "# 4. Backward pass: Propagate error backward through the network\n",
        "# 5. Update weights and biases using the computed gradients\n",
        "# 6. Repeat until convergence or max epochs reached\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MLP_Backprop_Sigmoid:\n",
        "    def __init__(self, n_inputs, hidden1_size, hidden2_size):\n",
        "        # Initialize network architecture\n",
        "        self.n_inputs = n_inputs\n",
        "        self.hidden1_size = hidden1_size\n",
        "        self.hidden2_size = hidden2_size\n",
        "\n",
        "        # Initialize weights and biases with random values\n",
        "        self.weights1 = np.random.randn(n_inputs, hidden1_size) * 0.01\n",
        "        self.bias1 = np.zeros((1, hidden1_size))\n",
        "\n",
        "        self.weights2 = np.random.randn(hidden1_size, hidden2_size) * 0.01\n",
        "        self.bias2 = np.zeros((1, hidden2_size))\n",
        "\n",
        "        self.weights3 = np.random.randn(hidden2_size, 1) * 0.01\n",
        "        self.bias3 = np.zeros((1, 1))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"Sigmoid activation function\"\"\"\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        \"\"\"Derivative of sigmoid function\"\"\"\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass through the network\"\"\"\n",
        "        # First hidden layer\n",
        "        self.z1 = np.dot(X, self.weights1) + self.bias1\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "\n",
        "        # Second hidden layer\n",
        "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "\n",
        "        # Output layer\n",
        "        self.z3 = np.dot(self.a2, self.weights3) + self.bias3\n",
        "        self.a3 = self.sigmoid(self.z3)\n",
        "\n",
        "        return self.a3\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate):\n",
        "        \"\"\"Backward pass to update weights and biases\"\"\"\n",
        "        m = X.shape[0]  # Number of training examples\n",
        "\n",
        "        # Output layer error\n",
        "        dz3 = output - y\n",
        "        dw3 = np.dot(self.a2.T, dz3) / m\n",
        "        db3 = np.sum(dz3, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Second hidden layer error\n",
        "        dz2 = np.dot(dz3, self.weights3.T) * self.sigmoid_derivative(self.a2)\n",
        "        dw2 = np.dot(self.a1.T, dz2) / m\n",
        "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "\n",
        "        # First hidden layer error\n",
        "        dz1 = np.dot(dz2, self.weights2.T) * self.sigmoid_derivative(self.a1)\n",
        "        dw1 = np.dot(X.T, dz1) / m\n",
        "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights3 -= learning_rate * dw3\n",
        "        self.bias3 -= learning_rate * db3\n",
        "\n",
        "        self.weights2 -= learning_rate * dw2\n",
        "        self.bias2 -= learning_rate * db2\n",
        "\n",
        "        self.weights1 -= learning_rate * dw1\n",
        "        self.bias1 -= learning_rate * db1\n",
        "\n",
        "    def train(self, X, y, learning_rate=0.1, epochs=1000):\n",
        "        \"\"\"Train the network using backpropagation\"\"\"\n",
        "        X = np.array(X)\n",
        "        y = np.array(y).reshape(-1, 1)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            output = self.forward(X)\n",
        "\n",
        "            # Backward pass and update weights\n",
        "            self.backward(X, y, output, learning_rate)\n",
        "\n",
        "            # Calculate and print error every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                error = np.mean(np.abs(output - y))\n",
        "                print(f\"Epoch {epoch}, Error: {error}\")\n",
        "\n",
        "        return epochs\n",
        "\n",
        "def experiment19():\n",
        "    # Get number of inputs from user\n",
        "    n_inputs = int(input(\"Enter number of binary inputs (N): \"))\n",
        "\n",
        "    # Configure network architecture\n",
        "    hidden1_size = n_inputs + 2  # Simple heuristic for hidden layer size\n",
        "    hidden2_size = n_inputs      # Second hidden layer slightly smaller\n",
        "\n",
        "    # Create MLP\n",
        "    mlp = MLP_Backprop_Sigmoid(n_inputs, hidden1_size, hidden2_size)\n",
        "\n",
        "    # Generate all possible binary input combinations\n",
        "    X = np.array([list(map(int, format(i, f'0{n_inputs}b'))) for i in range(2**n_inputs)])\n",
        "\n",
        "    # For demo purposes, use XOR for 2 inputs, or more complex function for more inputs\n",
        "    if n_inputs == 2:\n",
        "        # XOR function\n",
        "        y = np.array([int(sum(x) == 1) for x in X])\n",
        "    else:\n",
        "        # For more inputs, use a function that's 1 if more than half inputs are 1\n",
        "        y = np.array([int(sum(x) > n_inputs/2) for x in X])\n",
        "\n",
        "    # Train the MLP\n",
        "    epochs = mlp.train(X, y)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\nTraining completed in {epochs} epochs\")\n",
        "    print(\"\\nFinal weights and biases:\")\n",
        "    print(\"Layer 1 Weights:\")\n",
        "    print(mlp.weights1)\n",
        "    print(\"Layer 1 Bias:\")\n",
        "    print(mlp.bias1)\n",
        "    print(\"\\nLayer 2 Weights:\")\n",
        "    print(mlp.weights2)\n",
        "    print(\"Layer 2 Bias:\")\n",
        "    print(mlp.bias2)\n",
        "    print(\"\\nOutput Layer Weights:\")\n",
        "    print(mlp.weights3)\n",
        "    print(\"Output Layer Bias:\")\n",
        "    print(mlp.bias3)\n",
        "\n",
        "    # Test the network with all possible inputs\n",
        "    print(\"\\nTesting the network:\")\n",
        "    outputs = mlp.forward(X)\n",
        "    predicted = (outputs >= 0.5).astype(int)\n",
        "\n",
        "    for i, inputs in enumerate(X):\n",
        "        print(f\"Input: {inputs}, Raw Output: {outputs[i][0]:.4f}, Predicted: {predicted[i][0]}, Expected: {y[i]}\")\n",
        "\n",
        "# Run the experiment\n",
        "experiment19()"
      ],
      "metadata": {
        "id": "gmMshsnxYdnW"
      },
      "id": "gmMshsnxYdnW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 20: MULTI-LAYER PERCEPTRON - N BINARY INPUTS, TWO HIDDEN LAYERS, ONE OUTPUT (RELU)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# This experiment implements a Multi-Layer Perceptron with N binary inputs, two hidden layers,\n",
        "# and one output. Backpropagation is used to train the network with the ReLU (Rectified Linear Unit)\n",
        "# function as the activation function.\n",
        "#\n",
        "# Algorithm (Backpropagation with ReLU):\n",
        "# 1. Initialize random weights and biases\n",
        "# 2. Forward pass: Compute output of the network using ReLU activation for hidden layers\n",
        "# 3. Compute error at the output layer\n",
        "# 4. Backward pass: Propagate error backward through the network\n",
        "# 5. Update weights and biases using the computed gradients\n",
        "# 6. Repeat until convergence or max epochs reached\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MLP_Backprop_ReLU:\n",
        "    def __init__(self, n_inputs, hidden1_size, hidden2_size):\n",
        "        # Initialize network architecture\n",
        "        self.n_inputs = n_inputs\n",
        "        self.hidden1_size = hidden1_size\n",
        "        self.hidden2_size = hidden2_size\n",
        "\n",
        "        # Initialize weights and biases with random values\n",
        "        # He initialization for ReLU\n",
        "        self.weights1 = np.random.randn(n_inputs, hidden1_size) * np.sqrt(2.0/n_inputs)\n",
        "        self.bias1 = np.zeros((1, hidden1_size))\n",
        "\n",
        "        self.weights2 = np.random.randn(hidden1_size, hidden2_size) * np.sqrt(2.0/hidden1_size)\n",
        "        self.bias2 = np.zeros((1, hidden2_size))\n",
        "\n",
        "        self.weights3 = np.random.randn(hidden2_size, 1) * np.sqrt(2.0/hidden2_size)\n",
        "        self.bias3 = np.zeros((1, 1))\n",
        "\n",
        "    def relu(self, x):\n",
        "        \"\"\"ReLU activation function\"\"\"\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        \"\"\"Derivative of ReLU function\"\"\"\n",
        "        return np.where(x > 0, 1, 0)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"Sigmoid activation function for output layer\"\"\"\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        \"\"\"Derivative of sigmoid function\"\"\"\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass through the network\"\"\"\n",
        "        # First hidden layer with ReLU\n",
        "        self.z1 = np.dot(X, self.weights1) + self.bias1\n",
        "        self.a1 = self.relu(self.z1)\n",
        "\n",
        "        # Second hidden layer with ReLU\n",
        "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2\n",
        "        self.a2 = self.relu(self.z2)\n",
        "\n",
        "        # Output layer with sigmoid for binary classification\n",
        "        self.z3 = np.dot(self.a2, self.weights3) + self.bias3\n",
        "        self.a3 = self.sigmoid(self.z3)\n",
        "\n",
        "        return self.a3\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate):\n",
        "        \"\"\"Backward pass to update weights and biases\"\"\"\n",
        "        m = X.shape[0]  # Number of training examples\n",
        "\n",
        "        # Output layer error (using sigmoid derivative)\n",
        "        dz3 = output - y\n",
        "        dw3 = np.dot(self.a2.T, dz3) / m\n",
        "        db3 = np.sum(dz3, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Second hidden layer error (using ReLU derivative)\n",
        "        dz2 = np.dot(dz3, self.weights3.T) * self.relu_derivative(self.z2)\n",
        "        dw2 = np.dot(self.a1.T, dz2) / m\n",
        "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "\n",
        "        # First hidden layer error (using ReLU derivative)\n",
        "        dz1 = np.dot(dz2, self.weights2.T) * self.relu_derivative(self.z1)\n",
        "        dw1 = np.dot(X.T, dz1) / m\n",
        "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights3 -= learning_rate * dw3\n",
        "        self.bias3 -= learning_rate * db3\n",
        "\n",
        "        self.weights2 -= learning_rate * dw2\n",
        "        self.bias2 -= learning_rate * db2\n",
        "\n",
        "        self.weights1 -= learning_rate * dw1\n",
        "        self.bias1 -= learning_rate * db1\n",
        "\n",
        "    def train(self, X, y, learning_rate=0.01, epochs=2000):\n",
        "        \"\"\"Train the network using backpropagation\"\"\"\n",
        "        X = np.array(X)\n",
        "        y = np.array(y).reshape(-1, 1)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            output = self.forward(X)\n",
        "\n",
        "            # Backward pass and update weights\n",
        "            self.backward(X, y, output, learning_rate)\n",
        "\n",
        "            # Calculate and print error every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                error = np.mean(np.abs(output - y))\n",
        "                print(f\"Epoch {epoch}, Error: {error}\")\n",
        "\n",
        "        return epochs\n",
        "\n",
        "def experiment20():\n",
        "    # Get number of inputs from user\n",
        "    n_inputs = int(input(\"Enter number of binary inputs (N): \"))\n",
        "\n",
        "    # Configure network architecture\n",
        "    hidden1_size = n_inputs * 2  # ReLU typically needs more neurons\n",
        "    hidden2_size = n_inputs      # Second hidden layer\n",
        "\n",
        "    # Create MLP with ReLU\n",
        "    mlp = MLP_Backprop_ReLU(n_inputs, hidden1_size, hidden2_size)\n",
        "\n",
        "    # Generate all possible binary input combinations\n",
        "    X = np.array([list(map(int, format(i, f'0{n_inputs}b'))) for i in range(2**n_inputs)])\n",
        "\n",
        "    # For demo purposes, use a function that's 1 if odd number of 1's (parity function)\n",
        "    y = np.array([int(sum(x) % 2 == 1) for x in X])\n",
        "\n",
        "    # Train the MLP\n",
        "    epochs = mlp.train(X, y)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\nTraining completed in {epochs} epochs\")\n",
        "    print(\"\\nFinal weights and biases:\")\n",
        "    print(\"Layer 1 Weights:\")\n",
        "    print(mlp.weights1)\n",
        "    print(\"Layer 1 Bias:\")\n",
        "    print(mlp.bias1)\n",
        "    print(\"\\nLayer 2 Weights:\")\n",
        "    print(mlp.weights2)\n",
        "    print(\"Layer 2 Bias:\")\n",
        "    print(mlp.bias2)\n",
        "    print(\"\\nOutput Layer Weights:\")\n",
        "    print(mlp.weights3)\n",
        "    print(\"Output Layer Bias:\")\n",
        "    print(mlp.bias3)\n",
        "\n",
        "    # Test the network with all possible inputs\n",
        "    print(\"\\nTesting the network:\")\n",
        "    outputs = mlp.forward(X)\n",
        "    predicted = (outputs >= 0.5).astype(int)\n",
        "\n",
        "    for i, inputs in enumerate(X):\n",
        "        print(f\"Input: {inputs}, Raw Output: {outputs[i][0]:.4f}, Predicted: {predicted[i][0]}, Expected: {y[i]}\")\n",
        "\n",
        "# Run the experiment\n",
        "experiment20()"
      ],
      "metadata": {
        "id": "R2WTNGYeYjY5"
      },
      "id": "R2WTNGYeYjY5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 21: MLP - N BINARY INPUTS, TWO HIDDEN LAYERS, ONE OUTPUT (TANH) (KEYWORD: MLP_TANH)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# This MLP uses hyperbolic tangent (tanh) activation functions in the hidden layers,\n",
        "# which outputs values between -1 and 1. Tanh can help with faster convergence in some\n",
        "# cases compared to sigmoid. The output layer still uses sigmoid for binary classification.\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Initialize weights and biases with appropriate scaling for tanh\n",
        "# 2. Implement forward propagation:\n",
        "#    a. First hidden layer with tanh activation\n",
        "#    b. Second hidden layer with tanh activation\n",
        "#    c. Output layer with sigmoid activation\n",
        "# 3. Implement backpropagation:\n",
        "#    a. Compute gradients using tanh derivatives for hidden layers\n",
        "#    b. Update weights using gradient descent\n",
        "# 4. Train on all possible binary input combinations\n",
        "# 5. Display final weights and test performance\n",
        "import numpy as np\n",
        "\n",
        "class MLP_Backprop_Tanh:\n",
        "    def __init__(self, n_inputs, hidden1_size, hidden2_size):\n",
        "        self.n_inputs = n_inputs\n",
        "        self.hidden1_size = hidden1_size\n",
        "        self.hidden2_size = hidden2_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights1 = np.random.randn(n_inputs, hidden1_size) * 0.01\n",
        "        self.bias1 = np.zeros((1, hidden1_size))\n",
        "\n",
        "        self.weights2 = np.random.randn(hidden1_size, hidden2_size) * 0.01\n",
        "        self.bias2 = np.zeros((1, hidden2_size))\n",
        "\n",
        "        self.weights3 = np.random.randn(hidden2_size, 1) * 0.01\n",
        "        self.bias3 = np.zeros((1, 1))\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def tanh_derivative(self, x):\n",
        "        return 1 - np.tanh(x)**2\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # First hidden layer with tanh\n",
        "        self.z1 = np.dot(X, self.weights1) + self.bias1\n",
        "        self.a1 = self.tanh(self.z1)\n",
        "\n",
        "        # Second hidden layer with tanh\n",
        "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2\n",
        "        self.a2 = self.tanh(self.z2)\n",
        "\n",
        "        # Output layer with sigmoid\n",
        "        self.z3 = np.dot(self.a2, self.weights3) + self.bias3\n",
        "        self.a3 = self.sigmoid(self.z3)\n",
        "\n",
        "        return self.a3\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate):\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # Output layer error\n",
        "        dz3 = output - y\n",
        "        dw3 = np.dot(self.a2.T, dz3) / m\n",
        "        db3 = np.sum(dz3, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Second hidden layer error\n",
        "        dz2 = np.dot(dz3, self.weights3.T) * self.tanh_derivative(self.z2)\n",
        "        dw2 = np.dot(self.a1.T, dz2) / m\n",
        "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "\n",
        "        # First hidden layer error\n",
        "        dz1 = np.dot(dz2, self.weights2.T) * self.tanh_derivative(self.z1)\n",
        "        dw1 = np.dot(X.T, dz1) / m\n",
        "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights3 -= learning_rate * dw3\n",
        "        self.bias3 -= learning_rate * db3\n",
        "\n",
        "        self.weights2 -= learning_rate * dw2\n",
        "        self.bias2 -= learning_rate * db2\n",
        "\n",
        "        self.weights1 -= learning_rate * dw1\n",
        "        self.bias1 -= learning_rate * db1\n",
        "\n",
        "    def train(self, X, y, learning_rate=0.1, epochs=1000):\n",
        "        X = np.array(X)\n",
        "        y = np.array(y).reshape(-1, 1)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output, learning_rate)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                error = np.mean(np.abs(output - y))\n",
        "                print(f\"Epoch {epoch}, Error: {error}\")\n",
        "\n",
        "        return epochs\n",
        "\n",
        "def experiment21():\n",
        "    n_inputs = int(input(\"Enter number of binary inputs (N): \"))\n",
        "    hidden1_size = n_inputs + 2\n",
        "    hidden2_size = n_inputs\n",
        "\n",
        "    mlp = MLP_Backprop_Tanh(n_inputs, hidden1_size, hidden2_size)\n",
        "\n",
        "    # Generate all possible binary input combinations\n",
        "    X = np.array([list(map(int, format(i, f'0{n_inputs}b'))) for i in range(2**n_inputs)])\n",
        "\n",
        "    # Use XOR-like function for 2 inputs, parity function for more\n",
        "    if n_inputs == 2:\n",
        "        y = np.array([int(sum(x) == 1) for x in X])\n",
        "    else:\n",
        "        y = np.array([int(sum(x) % 2 == 1) for x in X])\n",
        "\n",
        "    # Train the network\n",
        "    epochs = mlp.train(X, y)\n",
        "\n",
        "    print(f\"\\nTraining completed in {epochs} epochs\")\n",
        "    print(\"\\nFinal weights and biases:\")\n",
        "    print(\"Layer 1 Weights:\\n\", mlp.weights1)\n",
        "    print(\"Layer 1 Bias:\\n\", mlp.bias1)\n",
        "    print(\"\\nLayer 2 Weights:\\n\", mlp.weights2)\n",
        "    print(\"Layer 2 Bias:\\n\", mlp.bias2)\n",
        "    print(\"\\nOutput Layer Weights:\\n\", mlp.weights3)\n",
        "    print(\"Output Layer Bias:\\n\", mlp.bias3)\n",
        "\n",
        "    print(\"\\nTesting the network:\")\n",
        "    outputs = mlp.forward(X)\n",
        "    predicted = (outputs >= 0.5).astype(int)\n",
        "\n",
        "    for i, inputs in enumerate(X):\n",
        "        print(f\"Input: {inputs}, Output: {outputs[i][0]:.4f}, Predicted: {predicted[i][0]}, Expected: {y[i]}\")\n",
        "\n",
        "experiment21()\n"
      ],
      "metadata": {
        "id": "1jVaf_eWaNhT"
      },
      "id": "1jVaf_eWaNhT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 22: TEXT PROCESSING PIPELINE (KEYWORD: TEXT_PROCESSING)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# Text preprocessing is crucial for NLP tasks. This pipeline demonstrates fundamental\n",
        "# text cleaning and normalization steps that convert raw text into a more analyzable form.\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Text cleaning:\n",
        "#    a. Remove punctuation and special characters using regex\n",
        "#    b. Remove numbers and extra whitespace\n",
        "#    c. Remove non-ASCII characters\n",
        "# 2. Case normalization:\n",
        "#    a. Convert all text to lowercase\n",
        "# 3. Tokenization:\n",
        "#    a. Split text into individual words/tokens\n",
        "# 4. Stopword removal:\n",
        "#    a. Filter out common words that carry little meaning\n",
        "# 5. Spelling correction:\n",
        "#    a. Identify and correct misspelled words\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "def experiment22():\n",
        "    # Read text file\n",
        "    with open('sample_text.txt', 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    print(\"Original text:\")\n",
        "    print(text[:500], \"...\")  # Print first 500 characters\n",
        "\n",
        "    # a. Text cleaning\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove text in brackets\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)  # Remove words with numbers\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove non-ASCII\n",
        "\n",
        "    print(\"\\nAfter cleaning:\")\n",
        "    print(text[:500], \"...\")\n",
        "\n",
        "    # b. Convert to lowercase\n",
        "    text = text.lower()\n",
        "    print(\"\\nAfter lowercase conversion:\")\n",
        "    print(text[:500], \"...\")\n",
        "\n",
        "    # c. Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    print(\"\\nTokens (first 50):\")\n",
        "    print(tokens[:50])\n",
        "\n",
        "    # d. Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    print(\"\\nAfter stopword removal (first 50):\")\n",
        "    print(filtered_tokens[:50])\n",
        "\n",
        "    # e. Correct misspelled words\n",
        "    spell = SpellChecker()\n",
        "    corrected_tokens = []\n",
        "    for word in filtered_tokens:\n",
        "        corrected = spell.correction(word)\n",
        "        if corrected is not None and corrected != word:\n",
        "            corrected_tokens.append(f\"{word}({corrected})\")\n",
        "        else:\n",
        "            corrected_tokens.append(word)\n",
        "\n",
        "    print(\"\\nWith spelling corrections:\")\n",
        "    print(corrected_tokens[:50])\n",
        "\n",
        "# Note: You'll need to install nltk and spellchecker packages\n",
        "# and download nltk data (stopwords, punkt) before running\n",
        "experiment22()"
      ],
      "metadata": {
        "id": "jhyMY4XQahFZ"
      },
      "id": "jhyMY4XQahFZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 23: TEXT PROCESSING WITH STEMMING/LEMMATIZATION (KEYWORD: TEXT_STEM_LEMMA)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# Stemming and lemmatization are text normalization techniques that reduce words to their\n",
        "# base or root forms. Stemming uses heuristic rules, while lemmatization uses vocabulary\n",
        "# and morphological analysis for more accurate results.\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Perform basic text cleaning (punctuation, numbers, etc.)\n",
        "# 2. Case normalization (lowercase conversion)\n",
        "# 3. Apply stemming (Porter Stemmer algorithm)\n",
        "# 4. Apply lemmatization (WordNet lemmatizer)\n",
        "# 5. Generate n-grams (3 consecutive words) from lemmatized tokens\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def experiment23():\n",
        "    # Read text file\n",
        "    with open('sample_text.txt', 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # a. Text cleaning\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "    # b. Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # c. Stemming and Lemmatization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Stemming\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_tokens = [ps.stem(word) for word in tokens]\n",
        "    print(\"Stemmed tokens (first 50):\")\n",
        "    print(stemmed_tokens[:50])\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    print(\"\\nLemmatized tokens (first 50):\")\n",
        "    print(lemmatized_tokens[:50])\n",
        "\n",
        "    # d. Create list of 3 consecutive words after lemmatization\n",
        "    trigrams = []\n",
        "    for i in range(len(lemmatized_tokens) - 2):\n",
        "        trigrams.append(f\"{lemmatized_tokens[i]} {lemmatized_tokens[i+1]} {lemmatized_tokens[i+2]}\")\n",
        "\n",
        "    print(\"\\nTrigrams (first 20):\")\n",
        "    print(trigrams[:20])\n",
        "\n",
        "# Note: Requires nltk and wordnet data\n",
        "experiment23()"
      ],
      "metadata": {
        "id": "9TarF9xXah3J"
      },
      "id": "9TarF9xXah3J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 24: ONE-HOT ENCODING FOR TECHNICAL TEXTS (KEYWORD: ONE_HOT_ENCODING)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# One-hot encoding represents text as binary vectors where each dimension corresponds to\n",
        "# a word in the vocabulary. A document is represented by a vector with 1s for present words\n",
        "# and 0s for absent words.\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Read multiple technical text documents\n",
        "# 2. Perform basic text cleaning\n",
        "# 3. Create vocabulary of unique words across all documents\n",
        "# 4. For each document:\n",
        "#    a. Create binary vector with length equal to vocabulary size\n",
        "#    b. Set vector elements to 1 for words present in document\n",
        "# 5. Display vocabulary and encoded vectors\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import glob\n",
        "\n",
        "def experiment24():\n",
        "    # Read 3 technical text files\n",
        "    files = glob.glob('technical_*.txt')\n",
        "    documents = []\n",
        "\n",
        "    for file in files[:3]:  # Process first 3 files\n",
        "        with open(file, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "            # Basic cleaning\n",
        "            text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "            documents.append(text)\n",
        "\n",
        "    # Create one-hot encoding\n",
        "    vectorizer = CountVectorizer(binary=True)\n",
        "    X = vectorizer.fit_transform(documents)\n",
        "\n",
        "    print(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n",
        "    print(\"\\nFeature names (first 50):\")\n",
        "    print(vectorizer.get_feature_names_out()[:50])\n",
        "\n",
        "    print(\"\\nOne-hot encoded matrix:\")\n",
        "    print(X.toarray())\n",
        "\n",
        "experiment24()"
      ],
      "metadata": {
        "id": "EA59ZzbSajpe"
      },
      "id": "EA59ZzbSajpe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 25: BAG OF WORDS FOR MOVIE REVIEWS (KEYWORD: BAG_OF_WORDS)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# Bag of Words (BoW) represents text as word frequency vectors, ignoring word order but\n",
        "# maintaining multiplicity. It's a simple but effective text representation for many NLP tasks.\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Read multiple movie review documents\n",
        "# 2. Perform text cleaning and tokenization\n",
        "# 3. Create vocabulary of unique words\n",
        "# 4. For each document:\n",
        "#    a. Count occurrences of each vocabulary word\n",
        "#    b. Create frequency vector\n",
        "# 5. Display vocabulary and BoW vectors\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import glob\n",
        "\n",
        "def experiment25():\n",
        "    # Read 3 movie review files\n",
        "    files = glob.glob('review_*.txt')\n",
        "    documents = []\n",
        "\n",
        "    for file in files[:3]:\n",
        "        with open(file, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "            # Basic cleaning\n",
        "            text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "            documents.append(text)\n",
        "\n",
        "    # Create bag of words\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(documents)\n",
        "\n",
        "    print(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n",
        "    print(\"\\nFeature names (first 50):\")\n",
        "    print(vectorizer.get_feature_names_out()[:50])\n",
        "\n",
        "    print(\"\\nBag of words matrix:\")\n",
        "    print(X.toarray())\n",
        "\n",
        "experiment25()"
      ],
      "metadata": {
        "id": "hTwOAlwnalKI"
      },
      "id": "hTwOAlwnalKI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# EXPERIMENT 26: TF-IDF FOR TOURIST PLACES (KEYWORD: TFIDF)\n",
        "# ===============================================\n",
        "# Theory:\n",
        "# TF-IDF (Term Frequency-Inverse Document Frequency) measures word importance by considering:\n",
        "# - Term Frequency (TF): how often a word appears in a document\n",
        "# - Inverse Document Frequency (IDF): how rare a word is across all documents\n",
        "# This helps highlight distinctive words in each document.\n",
        "#\n",
        "# Algorithm:\n",
        "# 1. Read multiple documents about tourist places\n",
        "# 2. Perform text preprocessing\n",
        "# 3. Calculate TF for each word in each document\n",
        "# 4. Calculate IDF for each word across all documents\n",
        "# 5. Compute TF-IDF scores (TF * IDF)\n",
        "# 6. Display vocabulary and TF-IDF vectors\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import glob\n",
        "\n",
        "def experiment26():\n",
        "    # Read 3 tourist place files\n",
        "    files = glob.glob('tourist_*.txt')\n",
        "    documents = []\n",
        "\n",
        "    for file in files[:3]:\n",
        "        with open(file, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "            # Basic cleaning\n",
        "            text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "            documents.append(text)\n",
        "\n",
        "    # Create TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(documents)\n",
        "\n",
        "    print(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n",
        "    print(\"\\nFeature names (first 50):\")\n",
        "    print(vectorizer.get_feature_names_out()[:50])\n",
        "\n",
        "    print(\"\\nTF-IDF matrix:\")\n",
        "    print(X.toarray())\n",
        "\n",
        "experiment26()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "x571C0VWc6Sn",
        "outputId": "32ee5f82-8144-4e6f-a8bc-5a3fa1055f56"
      },
      "id": "x571C0VWc6Sn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "empty vocabulary; perhaps the documents only contain stop words",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ea684ac10dfd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mexperiment26\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-ea684ac10dfd>\u001b[0m in \u001b[0;36mexperiment26\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Create TF-IDF vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabulary size:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2102\u001b[0m             \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m         )\n\u001b[0;32m-> 2104\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1374\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1283\u001b[0m                     \u001b[0;34m\"empty vocabulary; perhaps the documents only contain stop words\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GtA5QBEMc85W"
      },
      "id": "GtA5QBEMc85W",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}